{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark-Regression\n",
    "**PySpark** comes with a very powerful **MachineLearning** library called **MLLib**. In the notebook below we will use regression functions of MLLib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import PySpark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets start a spark session\n",
    "spark = SparkSession.builder.appName('regression').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Seoul bike data\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../data/SeoulBikeData/SeoulBikeData.csv',\n",
    "                    header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+----+---------------+-----------+----------------+----------------+-------------------------+-----------------------+------------+-------------+-------+----------+---------------+\n",
      "|      Date|Rented Bike Count|Hour|Temperature(�C)|Humidity(%)|Wind speed (m/s)|Visibility (10m)|Dew point temperature(�C)|Solar Radiation (MJ/m2)|Rainfall(mm)|Snowfall (cm)|Seasons|   Holiday|Functioning Day|\n",
      "+----------+-----------------+----+---------------+-----------+----------------+----------------+-------------------------+-----------------------+------------+-------------+-------+----------+---------------+\n",
      "|01/12/2017|              254|   0|           -5.2|         37|             2.2|            2000|                    -17.6|                      0|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              204|   1|           -5.5|         38|             0.8|            2000|                    -17.6|                      0|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              173|   2|             -6|         39|               1|            2000|                    -17.7|                      0|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              107|   3|           -6.2|         40|             0.9|            2000|                    -17.6|                      0|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|               78|   4|             -6|         36|             2.3|            2000|                    -18.6|                      0|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              100|   5|           -6.4|         37|             1.5|            2000|                    -18.7|                      0|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              181|   6|           -6.6|         35|             1.3|            2000|                    -19.5|                      0|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              460|   7|           -7.4|         38|             0.9|            2000|                    -19.3|                      0|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              930|   8|           -7.6|         37|             1.1|            2000|                    -19.8|                   0.01|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              490|   9|           -6.5|         27|             0.5|            1928|                    -22.4|                   0.23|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              339|  10|           -3.5|         24|             1.2|            1996|                    -21.2|                   0.65|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              360|  11|           -0.5|         21|             1.3|            1936|                    -20.2|                   0.94|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              449|  12|            1.7|         23|             1.4|            2000|                    -17.2|                   1.11|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              451|  13|            2.4|         25|             1.6|            2000|                    -15.6|                   1.16|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              447|  14|              3|         26|               2|            2000|                    -14.6|                   1.01|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              463|  15|            2.1|         36|             3.2|            2000|                    -11.4|                   0.54|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              484|  16|            1.2|         54|             4.2|             793|                       -7|                   0.24|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              555|  17|            0.8|         58|             1.6|            2000|                     -6.5|                   0.08|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              862|  18|            0.6|         66|             1.4|            2000|                       -5|                      0|           0|            0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              600|  19|              0|         77|             1.7|            2000|                     -3.5|                      0|           0|            0| Winter|No Holiday|            Yes|\n",
      "+----------+-----------------+----+---------------+-----------+----------------+----------------+-------------------------+-----------------------+------------+-------------+-------+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the data\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Rented Bike Count: string (nullable = true)\n",
      " |-- Hour: string (nullable = true)\n",
      " |-- Temperature(�C): string (nullable = true)\n",
      " |-- Humidity(%): string (nullable = true)\n",
      " |-- Wind speed (m/s): string (nullable = true)\n",
      " |-- Visibility (10m): string (nullable = true)\n",
      " |-- Dew point temperature(�C): string (nullable = true)\n",
      " |-- Solar Radiation (MJ/m2): string (nullable = true)\n",
      " |-- Rainfall(mm): string (nullable = true)\n",
      " |-- Snowfall (cm): string (nullable = true)\n",
      " |-- Seasons: string (nullable = true)\n",
      " |-- Holiday: string (nullable = true)\n",
      " |-- Functioning Day: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printSchema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note Spark infered all the columns as strings, below we will convert numeric columns to numeric types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Rented Bike Count', 'Hour', 'Temperature(�C)', 'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)', 'Dew point temperature(�C)', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)', 'Seasons', 'Holiday', 'Functioning Day']\n"
     ]
    }
   ],
   "source": [
    "#Get the column names\n",
    "print(df.columns)\n",
    "\n",
    "#Rename some Columns\n",
    "df = df.withColumnRenamed('Temperature(�C)','Temperature') \\\n",
    "        .withColumnRenamed('Humidity(%)','Humidity') \\\n",
    "        .withColumnRenamed('Dew point temperature(�C)', 'Dew point temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Rented Bike Count', 'Hour', 'Temperature', 'Humidity', 'Wind speed (m/s)', 'Visibility (10m)', 'Dew point temperature', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)', 'Seasons', 'Holiday', 'Functioning Day']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the data to the types we want\n",
    "from pyspark.sql.types import (StructField, \n",
    "                               StringType, \n",
    "                               IntegerType,\n",
    "                               DateType,\n",
    "                               DoubleType,\n",
    "                               StructType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField('Date', StringType(), True), \n",
    "               StructField('Rented Bike Count', IntegerType(), True),\n",
    "               StructField('Hour', IntegerType(), True),\n",
    "               StructField('Temperature', DoubleType(), True),\n",
    "               StructField('Humidity', DoubleType(), True),\n",
    "               StructField('Wind speed (m/s)', DoubleType(), True),\n",
    "               StructField('Visibility (10m)', DoubleType(), True),\n",
    "               StructField('Dew point temperature', DoubleType(), True),\n",
    "               StructField('Solar Radiation (MJ/m2)', DoubleType(), True),\n",
    "               StructField('Rainfall(mm)', DoubleType(), True),\n",
    "               StructField('Snowfall (cm)', DoubleType(), True),\n",
    "               StructField('Seasons', StringType(), True),\n",
    "               StructField('Holiday', StringType(), True),\n",
    "               StructField('Functioning Day',  StringType(), True)\n",
    "              ]\n",
    "final_struct = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the data again with correct data types in schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../data/SeoulBikeData/SeoulBikeData.csv', header=True, schema=final_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+----+-----------+--------+----------------+----------------+---------------------+-----------------------+------------+-------------+-------+----------+---------------+\n",
      "|      Date|Rented Bike Count|Hour|Temperature|Humidity|Wind speed (m/s)|Visibility (10m)|Dew point temperature|Solar Radiation (MJ/m2)|Rainfall(mm)|Snowfall (cm)|Seasons|   Holiday|Functioning Day|\n",
      "+----------+-----------------+----+-----------+--------+----------------+----------------+---------------------+-----------------------+------------+-------------+-------+----------+---------------+\n",
      "|01/12/2017|              254|   0|       -5.2|    37.0|             2.2|          2000.0|                -17.6|                    0.0|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              204|   1|       -5.5|    38.0|             0.8|          2000.0|                -17.6|                    0.0|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              173|   2|       -6.0|    39.0|             1.0|          2000.0|                -17.7|                    0.0|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              107|   3|       -6.2|    40.0|             0.9|          2000.0|                -17.6|                    0.0|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|               78|   4|       -6.0|    36.0|             2.3|          2000.0|                -18.6|                    0.0|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              100|   5|       -6.4|    37.0|             1.5|          2000.0|                -18.7|                    0.0|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              181|   6|       -6.6|    35.0|             1.3|          2000.0|                -19.5|                    0.0|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              460|   7|       -7.4|    38.0|             0.9|          2000.0|                -19.3|                    0.0|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              930|   8|       -7.6|    37.0|             1.1|          2000.0|                -19.8|                   0.01|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              490|   9|       -6.5|    27.0|             0.5|          1928.0|                -22.4|                   0.23|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              339|  10|       -3.5|    24.0|             1.2|          1996.0|                -21.2|                   0.65|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              360|  11|       -0.5|    21.0|             1.3|          1936.0|                -20.2|                   0.94|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              449|  12|        1.7|    23.0|             1.4|          2000.0|                -17.2|                   1.11|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              451|  13|        2.4|    25.0|             1.6|          2000.0|                -15.6|                   1.16|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              447|  14|        3.0|    26.0|             2.0|          2000.0|                -14.6|                   1.01|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              463|  15|        2.1|    36.0|             3.2|          2000.0|                -11.4|                   0.54|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              484|  16|        1.2|    54.0|             4.2|           793.0|                 -7.0|                   0.24|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              555|  17|        0.8|    58.0|             1.6|          2000.0|                 -6.5|                   0.08|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              862|  18|        0.6|    66.0|             1.4|          2000.0|                 -5.0|                    0.0|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "|01/12/2017|              600|  19|        0.0|    77.0|             1.7|          2000.0|                 -3.5|                    0.0|         0.0|          0.0| Winter|No Holiday|            Yes|\n",
      "+----------+-----------------+----+-----------+--------+----------------+----------------+---------------------+-----------------------+------------+-------------+-------+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+-----------------+------------------+------------------+------------------+-----------------+---------------------+-----------------------+------------------+-------------------+-------+----------+---------------+\n",
      "|summary|      Date|Rented Bike Count|             Hour|       Temperature|          Humidity|  Wind speed (m/s)| Visibility (10m)|Dew point temperature|Solar Radiation (MJ/m2)|      Rainfall(mm)|      Snowfall (cm)|Seasons|   Holiday|Functioning Day|\n",
      "+-------+----------+-----------------+-----------------+------------------+------------------+------------------+-----------------+---------------------+-----------------------+------------------+-------------------+-------+----------+---------------+\n",
      "|  count|      8760|             8760|             8760|              8760|              8760|              8760|             8760|                 8760|                   8760|              8760|               8760|   8760|      8760|           8760|\n",
      "|   mean|      null|704.6020547945205|             11.5|12.882922374429233|58.226255707762554|1.7249086757990943|1436.825799086758|    4.073812785388147|     0.5691107305936114|0.1486872146118721|0.07506849315068483|   null|      null|           null|\n",
      "| stddev|      null|644.9974677392169|6.922581688234331|11.944825230027929|20.362413301565603|1.0362999934025563|608.2987119840195|   13.060369338149783|     0.8687462422391529|1.1281929687321488|0.43674618112499025|   null|      null|           null|\n",
      "|    min|01/01/2018|                0|                0|             -17.8|               0.0|               0.0|             27.0|                -30.6|                    0.0|               0.0|                0.0| Autumn|   Holiday|             No|\n",
      "|    max|31/12/2017|             3556|               23|              39.4|              98.0|               7.4|           2000.0|                 27.2|                   3.52|              35.0|                8.8| Winter|No Holiday|            Yes|\n",
      "+-------+----------+-----------------+-----------------+------------------+------------------+------------------+-----------------+---------------------+-----------------------+------------------+-------------------+-------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Describe the dataframe for all the numerical columns\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Rented Bike Count: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Humidity: double (nullable = true)\n",
      " |-- Wind speed (m/s): double (nullable = true)\n",
      " |-- Visibility (10m): double (nullable = true)\n",
      " |-- Dew point temperature: double (nullable = true)\n",
      " |-- Solar Radiation (MJ/m2): double (nullable = true)\n",
      " |-- Rainfall(mm): double (nullable = true)\n",
      " |-- Snowfall (cm): double (nullable = true)\n",
      " |-- Seasons: string (nullable = true)\n",
      " |-- Holiday: string (nullable = true)\n",
      " |-- Functioning Day: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Double check the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Date to dateType in PySpark\n",
    "from pyspark.sql.functions import to_date\n",
    "df = df.withColumn('New_date', to_date(df['Date'],format='dd/MM/yyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Rented Bike Count: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Humidity: double (nullable = true)\n",
      " |-- Wind speed (m/s): double (nullable = true)\n",
      " |-- Visibility (10m): double (nullable = true)\n",
      " |-- Dew point temperature: double (nullable = true)\n",
      " |-- Solar Radiation (MJ/m2): double (nullable = true)\n",
      " |-- Rainfall(mm): double (nullable = true)\n",
      " |-- Snowfall (cm): double (nullable = true)\n",
      " |-- Seasons: string (nullable = true)\n",
      " |-- Holiday: string (nullable = true)\n",
      " |-- Functioning Day: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop old date column\n",
    "df = df.drop(df['Date'])\n",
    "#rename New_date as date\n",
    "df = df.withColumnRenamed('New_Date', 'Date')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import vector and VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assemble the features vector\n",
    "assembler = VectorAssembler(inputCols=['Hour',\n",
    "                                       'Temperature',\n",
    "                                       'Humidity',\n",
    "                                       'Wind speed (m/s)',\n",
    "                                       'Visibility (10m)', \n",
    "                                       'Dew point temperature',\n",
    "                                       'Solar Radiation (MJ/m2)',\n",
    "                                       'Rainfall(mm)',\n",
    "                                       'Snowfall (cm)'], \n",
    "                           outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.0,-5.2,37.0,2....|  254|\n",
      "|[1.0,-5.5,38.0,0....|  204|\n",
      "|[2.0,-6.0,39.0,1....|  173|\n",
      "|[3.0,-6.2,40.0,0....|  107|\n",
      "|[4.0,-6.0,36.0,2....|   78|\n",
      "|[5.0,-6.4,37.0,1....|  100|\n",
      "|[6.0,-6.6,35.0,1....|  181|\n",
      "|[7.0,-7.4,38.0,0....|  460|\n",
      "|[8.0,-7.6,37.0,1....|  930|\n",
      "|[9.0,-6.5,27.0,0....|  490|\n",
      "|[10.0,-3.5,24.0,1...|  339|\n",
      "|[11.0,-0.5,21.0,1...|  360|\n",
      "|[12.0,1.7,23.0,1....|  449|\n",
      "|[13.0,2.4,25.0,1....|  451|\n",
      "|[14.0,3.0,26.0,2....|  447|\n",
      "|[15.0,2.1,36.0,3....|  463|\n",
      "|[16.0,1.2,54.0,4....|  484|\n",
      "|[17.0,0.8,58.0,1....|  555|\n",
      "|[18.0,0.6,66.0,1....|  862|\n",
      "|[19.0,0.0,77.0,1....|  600|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final = output.select(['features', 'Rented Bike Count'])\n",
    "#Rename the Rented Bike Count as label\n",
    "df_final = df_final.withColumnRenamed('Rented Bike Count', 'label')\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "train_data, test_data = df_final.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr_model = lr.fit(train_data)\n",
    "test_result = lr_model.evaluate(test_data)\n",
    "train_result = lr_model.evaluate(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Result =  0.471\n",
      "Train Result = 0.47\n"
     ]
    }
   ],
   "source": [
    "#Lets print r2\n",
    "print(f\"Test Result = {test_result.r2: 0.3}\")\n",
    "print(f\"Train Result = {train_result.r2 :0.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try L1, L2 and Elastic Net regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization Parameter for bestModel 2.0\n",
      "Test result = 0.47\n",
      "Train result = 0.47\n"
     ]
    }
   ],
   "source": [
    "#L2\n",
    "#Lets setup a grid search to search for regualization parameter\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "lr = LinearRegression()\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [5, 3, 2.0, 1, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=5) \n",
    "cv_model = crossval.fit(train_data)\n",
    "test_result = cv_model.transform(test_data)\n",
    "train_result = cv_model.transform(train_data)\n",
    "\n",
    "#get regularization parameter for best Model \n",
    "print(f\"Regularization Parameter for bestModel {cv_model.bestModel.getOrDefault('regParam')}\")\n",
    "\n",
    "#Evaluate the model\n",
    "re = RegressionEvaluator()\n",
    "print(f\"Test result = {re.evaluate(test_result, {re.metricName: 'r2'}):0.3}\")\n",
    "print(f\"Train result = {re.evaluate(train_result, {re.metricName: 'r2'}):0.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization Parameter for bestModel 0.1\n",
      "Test result = 0.471\n",
      "Train result = 0.47\n"
     ]
    }
   ],
   "source": [
    "#L1\n",
    "#Lets setup a grid search to search for regualization parameter\n",
    "\n",
    "lr = LinearRegression(elasticNetParam=1.0)\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [5, 3, 2.0, 1, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=5) \n",
    "cv_model = crossval.fit(train_data)\n",
    "test_result = cv_model.transform(test_data)\n",
    "train_result = cv_model.transform(train_data)\n",
    "\n",
    "#get regularization parameter for best Model \n",
    "print(f\"Regularization Parameter for bestModel {cv_model.bestModel.getOrDefault('regParam')}\")\n",
    "\n",
    "#Evaluate the model\n",
    "re = RegressionEvaluator()\n",
    "print(f\"Test result = {re.evaluate(test_result, {re.metricName: 'r2'}):0.3}\")\n",
    "print(f\"Train result = {re.evaluate(train_result, {re.metricName: 'r2'}):0.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization Parameter for bestModel 1.0\n",
      "Elastic Net Parameter for bestModel 0.25\n",
      "Test result = 0.471\n",
      "Train result = 0.47\n"
     ]
    }
   ],
   "source": [
    "#elastic net\n",
    "#Lets setup a grid search to search for regualization parameter\n",
    "\n",
    "lr = LinearRegression()\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [5, 3, 2.0, 1, 0.1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0, 0.25, 0.5, 0.75, 1]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=5) \n",
    "cv_model = crossval.fit(train_data)\n",
    "test_result = cv_model.transform(test_data)\n",
    "train_result = cv_model.transform(train_data)\n",
    "\n",
    "#get regularization parameter for best Model \n",
    "print(f\"Regularization Parameter for bestModel {cv_model.bestModel.getOrDefault('regParam')}\")\n",
    "print(f\"Elastic Net Parameter for bestModel {cv_model.bestModel.getOrDefault('elasticNetParam')}\")\n",
    "\n",
    "#Evaluate the model\n",
    "re = RegressionEvaluator()\n",
    "print(f\"Test result = {re.evaluate(test_result, {re.metricName: 'r2'}):0.3}\")\n",
    "print(f\"Train result = {re.evaluate(train_result, {re.metricName: 'r2'}):0.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try tree based methods\n",
    "1. Decision Tree regressor\n",
    "2. Random forest regressor\n",
    "3. Gradient boosted trees regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import ( DecisionTreeRegressor,\n",
    "                                   RandomForestRegressor,\n",
    "                                   GBTRegressor )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make three models DecisionTree\n",
    "dtr = DecisionTreeRegressor()\n",
    "#Random Forest\n",
    "rfr = RandomForestRegressor(numTrees=100)\n",
    "#Gradient Boosting trees\n",
    "gbt = GBTRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train three models\n",
    "dtr_model = dtr.fit(train_data)\n",
    "rfr_model = rfr.fit(train_data)\n",
    "gbt_model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on test and train\n",
    "#Decision Tree\n",
    "dtr_train_result = dtr_model.transform(train_data)\n",
    "dtr_test_result = dtr_model.transform(test_data)\n",
    "\n",
    "#Random Forest\n",
    "rfr_train_result = rfr_model.transform(train_data)\n",
    "rfr_test_result = rfr_model.transform(test_data)\n",
    "\n",
    "#Gradient Boosting trees\n",
    "gbt_train_result = gbt_model.transform(train_data)\n",
    "gbt_test_result = gbt_model.transform(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|label|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(9,[1,2,4,5],[10....|  520|427.98591549295776|\n",
      "|(9,[1,2,4,5],[10....|  811|427.98591549295776|\n",
      "|[0.0,-15.1,34.0,1...|   68| 153.3186813186813|\n",
      "|[0.0,-15.0,42.0,1...|   80| 153.3186813186813|\n",
      "|[0.0,-14.6,34.0,3...|   90| 153.3186813186813|\n",
      "|[0.0,-13.6,49.0,1...|  105| 153.3186813186813|\n",
      "|[0.0,-12.3,47.0,0...|  116| 153.3186813186813|\n",
      "|[0.0,-11.0,51.0,1...|  133| 153.3186813186813|\n",
      "|[0.0,-10.6,47.0,3...|  126| 153.3186813186813|\n",
      "|[0.0,-10.5,52.0,2...|   82| 153.3186813186813|\n",
      "|[0.0,-10.0,34.0,1...|  108| 153.3186813186813|\n",
      "|[0.0,-9.3,45.0,0....|   80| 153.3186813186813|\n",
      "|[0.0,-7.7,52.0,3....|  103| 153.3186813186813|\n",
      "|[0.0,-7.5,36.0,2....|  125| 153.3186813186813|\n",
      "|[0.0,-6.6,39.0,1....|  152| 153.3186813186813|\n",
      "|[0.0,-6.5,41.0,1....|  164| 153.3186813186813|\n",
      "|[0.0,-6.3,54.0,1....|  159| 153.3186813186813|\n",
      "|[0.0,-6.1,42.0,2....|  216| 153.3186813186813|\n",
      "|[0.0,-5.9,50.0,0....|  151| 153.3186813186813|\n",
      "|[0.0,-5.2,37.0,2....|  254| 153.3186813186813|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtr_train_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|label|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(9,[1,2,4,5],[10....|  520| 406.5634315035093|\n",
      "|(9,[1,2,4,5],[10....|  811|416.90761163956415|\n",
      "|[0.0,-15.1,34.0,1...|   68|144.19399232047175|\n",
      "|[0.0,-15.0,42.0,1...|   80|145.60601243567385|\n",
      "|[0.0,-14.6,34.0,3...|   90|134.54948943527427|\n",
      "|[0.0,-13.6,49.0,1...|  105| 163.5901133627709|\n",
      "|[0.0,-12.3,47.0,0...|  116|163.09645903955285|\n",
      "|[0.0,-11.0,51.0,1...|  133|167.57968952304665|\n",
      "|[0.0,-10.6,47.0,3...|  126|153.01855581003667|\n",
      "|[0.0,-10.5,52.0,2...|   82|167.48448005869054|\n",
      "|[0.0,-10.0,34.0,1...|  108|163.23717326144273|\n",
      "|[0.0,-9.3,45.0,0....|   80| 163.4447957441462|\n",
      "|[0.0,-7.7,52.0,3....|  103| 165.0236005173475|\n",
      "|[0.0,-7.5,36.0,2....|  125| 158.6643555613476|\n",
      "|[0.0,-6.6,39.0,1....|  152|167.40222305363866|\n",
      "|[0.0,-6.5,41.0,1....|  164|167.83997710731492|\n",
      "|[0.0,-6.3,54.0,1....|  159|173.58570526955975|\n",
      "|[0.0,-6.1,42.0,2....|  216|159.89988403576055|\n",
      "|[0.0,-5.9,50.0,0....|  151|175.18073934581457|\n",
      "|[0.0,-5.2,37.0,2....|  254|168.82342696754947|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfr_train_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|label|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(9,[1,2,4,5],[10....|  520| 527.2576625879724|\n",
      "|(9,[1,2,4,5],[10....|  811| 566.5788039284892|\n",
      "|[0.0,-15.1,34.0,1...|   68|129.33541458918037|\n",
      "|[0.0,-15.0,42.0,1...|   80|129.33541458918037|\n",
      "|[0.0,-14.6,34.0,3...|   90|117.59318666922785|\n",
      "|[0.0,-13.6,49.0,1...|  105|137.68541458918034|\n",
      "|[0.0,-12.3,47.0,0...|  116|137.68541458918034|\n",
      "|[0.0,-11.0,51.0,1...|  133|137.68541458918034|\n",
      "|[0.0,-10.6,47.0,3...|  126|125.94318666922784|\n",
      "|[0.0,-10.5,52.0,2...|   82|135.21798784866405|\n",
      "|[0.0,-10.0,34.0,1...|  108|137.68541458918034|\n",
      "|[0.0,-9.3,45.0,0....|   80|145.64147787897505|\n",
      "|[0.0,-7.7,52.0,3....|  103|151.61382938891398|\n",
      "|[0.0,-7.5,36.0,2....|  125| 132.5936126216437|\n",
      "|[0.0,-6.6,39.0,1....|  152|159.78898685232826|\n",
      "|[0.0,-6.5,41.0,1....|  164|159.78898685232826|\n",
      "|[0.0,-6.3,54.0,1....|  159|159.78898685232826|\n",
      "|[0.0,-6.1,42.0,2....|  216|163.56646226796227|\n",
      "|[0.0,-5.9,50.0,0....|  151|176.61432752529362|\n",
      "|[0.0,-5.2,37.0,2....|  254|166.97581204877702|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbt_train_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result = 0.665\n",
      "Train result = 0.636\n",
      "Test result = 0.674\n",
      "Train result = 0.655\n",
      "Test result = 0.78\n",
      "Train result = 0.741\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "re = RegressionEvaluator()\n",
    "\n",
    "#Decision Tree\n",
    "print(f\"Test result = {re.evaluate(dtr_train_result, {re.metricName: 'r2'}):0.3}\")\n",
    "print(f\"Train result = {re.evaluate(dtr_test_result, {re.metricName: 'r2'}):0.3}\")\n",
    "\n",
    "#Random Forest\n",
    "print(f\"Test result = {re.evaluate(rfr_train_result, {re.metricName: 'r2'}):0.3}\")\n",
    "print(f\"Train result = {re.evaluate(rfr_test_result, {re.metricName: 'r2'}):0.3}\")\n",
    "\n",
    "#Gradient Boosting trees\n",
    "print(f\"Test result = {re.evaluate(gbt_train_result, {re.metricName: 'r2'}):0.3}\")\n",
    "print(f\"Train result = {re.evaluate(gbt_test_result, {re.metricName: 'r2'}):0.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(9, {0: 0.3485, 1: 0.4369, 2: 0.0332, 3: 0.0029, 4: 0.0091, 5: 0.0102, 6: 0.0661, 7: 0.093})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtr_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(9, {0: 0.3076, 1: 0.3053, 2: 0.0926, 3: 0.0097, 4: 0.0217, 5: 0.124, 6: 0.0753, 7: 0.0584, 8: 0.0052})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(9, {0: 0.3092, 1: 0.305, 2: 0.0767, 3: 0.0329, 4: 0.0428, 5: 0.0703, 6: 0.1184, 7: 0.0445, 8: 0.0001})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbt_model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from above feature 1 or Temperature seems to be most important feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above all the methods gave better results than Linear regression. Gradient Boosting trees beating it the most."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
