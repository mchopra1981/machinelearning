{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "equal-control",
   "metadata": {},
   "source": [
    "## ELMO\n",
    "\n",
    "**ELMO** is a word representation method that is used to represent words as vectors. Word vectors are learned in **ELMO** as a function of the internal states of a deep bidirectional language model, which is pre-trained on a large text corpus. \n",
    "\n",
    "**ELMO** is described in the following paper\n",
    "https://arxiv.org/abs/1802.05365\n",
    "\n",
    "In the sheet below we will use **ELMO** word vectors for sentiment classification of a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "immediate-verse",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:08:24.613351Z",
     "start_time": "2021-03-02T20:08:19.700117Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-flower",
   "metadata": {},
   "source": [
    "### Load and preprocess tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "little-soccer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:08:24.931129Z",
     "start_time": "2021-03-02T20:08:24.616747Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/jamieott/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jamieott/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples \n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adaptive-closer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:08:26.343332Z",
     "start_time": "2021-03-02T20:08:24.969053Z"
    }
   },
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "suitable-police",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:08:26.369092Z",
     "start_time": "2021-03-02T20:08:26.347317Z"
    }
   },
   "outputs": [],
   "source": [
    "#Train test split\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = np.array(train_pos + train_neg)\n",
    "test_x = np.array(test_pos + test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fatal-ensemble",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:08:26.387020Z",
     "start_time": "2021-03-02T20:08:26.374871Z"
    }
   },
   "outputs": [],
   "source": [
    "# combine positive and negative labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "threatened-chosen",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:08:26.422829Z",
     "start_time": "2021-03-02T20:08:26.389588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "given-console",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:08:26.507396Z",
     "start_time": "2021-03-02T20:08:26.454684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print(train_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "muslim-values",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:08:26.577185Z",
     "start_time": "2021-03-02T20:08:26.551293Z"
    }
   },
   "outputs": [],
   "source": [
    "## Process tweet\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, twitter_samples \n",
    "\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "stopwords_english = stopwords.words('english')\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "            word not in string.punctuation): # remove punctuation\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "    return ' '.join(tweets_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "classical-berry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:08:26.610639Z",
     "start_time": "2021-03-02T20:08:26.585521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'followfriday top engag member commun week :)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_tweet(train_pos[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-toner",
   "metadata": {},
   "source": [
    "### Load ELMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "working-recommendation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:08:36.293216Z",
     "start_time": "2021-03-02T20:08:26.619740Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load ELMO model from tensor flow hub\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-organizer",
   "metadata": {},
   "source": [
    "In the example below **ELMO** gives different word embeddings for the word fall depending upon context. In one context word fall is used as a weather in second context word fall is used as a verb (to fall). We can see cosine similarity of the word fall as weather is higher with word winter then with verb fall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "rapid-computer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:08:36.986885Z",
     "start_time": "2021-03-02T20:08:36.295890Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = elmo.signatures[\"default\"](tf.constant([\n",
    "                \"I love cool crisp fall weather\",\n",
    "                \"Dont fall on your way to the gym\",\n",
    "                \"winter\",\n",
    "                \"slip\"\n",
    "                ])\n",
    "                )[\"elmo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "restricted-subcommittee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:08:36.993882Z",
     "start_time": "2021-03-02T20:08:36.989419Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings_arr = embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fossil-measure",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:08:37.007876Z",
     "start_time": "2021-03-02T20:08:37.001277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8, 1024)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "local-guyana",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:08:37.033141Z",
     "start_time": "2021-03-02T20:08:37.019919Z"
    }
   },
   "outputs": [],
   "source": [
    "fall1 = embeddings_arr[0][4]\n",
    "fall2 = embeddings_arr[1][2]\n",
    "winter = embeddings_arr[2][0]\n",
    "slip = embeddings_arr[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unauthorized-thompson",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:08:37.056017Z",
     "start_time": "2021-03-02T20:08:37.039682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between fall (weather) and fall (verb) 0.156\n",
      "Cosine similarity between fall (weather) and winter 0.459\n",
      "Cosine similarity between fall (verb) and slip 0.187\n"
     ]
    }
   ],
   "source": [
    "##Now lets see consine similarity between these embeddings\n",
    "def cosine_similarity(A, B):\n",
    "    dot = np.dot(A,B)\n",
    "    norma = np.linalg.norm(A)\n",
    "    normb = np.linalg.norm(B)\n",
    "    cos = dot/(norma*normb)\n",
    "    return cos\n",
    "\n",
    "print(f\"Cosine similarity between fall (weather) and fall (verb) {cosine_similarity(fall1, fall2) :.3}\")\n",
    "print(f\"Cosine similarity between fall (weather) and winter {cosine_similarity(fall1, winter) :.3}\")\n",
    "print(f\"Cosine similarity between fall (verb) and slip {cosine_similarity(fall2, slip) :.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-museum",
   "metadata": {},
   "source": [
    "Lets prepare ELMO word vectors for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "irish-report",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:08:37.070871Z",
     "start_time": "2021-03-02T20:08:37.063002Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_embeddings(tweets):\n",
    "    embeddings = elmo.signatures[\"default\"](tf.convert_to_tensor(tweets))[\"elmo\"]\n",
    "    return tf.reduce_mean(embeddings,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pursuant-quarter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:08:37.102956Z",
     "start_time": "2021-03-02T20:08:37.080117Z"
    }
   },
   "outputs": [],
   "source": [
    "list_train_x = [train_x[i:i+100] for i in range(0,train_x.shape[0],100)]\n",
    "list_test_x = [test_x[i:i+100] for i in range(0,test_x.shape[0],100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "attractive-taste",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:20:44.083669Z",
     "start_time": "2021-03-02T20:08:37.111950Z"
    }
   },
   "outputs": [],
   "source": [
    "elmo_train_x = [create_embeddings(x) for x in list_train_x]\n",
    "elmo_test_x  = [create_embeddings(x) for x in list_test_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caroline-kelly",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:20:44.177595Z",
     "start_time": "2021-03-02T20:20:44.100913Z"
    }
   },
   "outputs": [],
   "source": [
    "elmo_train_x = np.concatenate(elmo_train_x, axis = 0)\n",
    "elmo_test_x = np.concatenate(elmo_test_x, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-visitor",
   "metadata": {},
   "source": [
    "Now lets apply Logistic regression to fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sublime-ethiopia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:20:44.481981Z",
     "start_time": "2021-03-02T20:20:44.181282Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(elmo_train_x)\n",
    "elmo_train_std_x = sc.transform(elmo_train_x)\n",
    "elmo_test_std_x  = sc.transform(elmo_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "involved-composer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:20:46.719189Z",
     "start_time": "2021-03-02T20:20:44.486430Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter=10000)\n",
    "lr.fit(elmo_train_std_x, train_y.reshape(train_y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "specific-newsletter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:20:46.827560Z",
     "start_time": "2021-03-02T20:20:46.722465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of train set 1.0\n",
      "Score in test set 0.951\n"
     ]
    }
   ],
   "source": [
    "print(f\"Score of train set {lr.score(elmo_train_std_x, train_y):.3}\")\n",
    "print(f\"Score in test set {lr.score(elmo_test_std_x, test_y):.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-language",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T20:07:09.912668Z",
     "start_time": "2021-03-02T20:07:09.901128Z"
    }
   },
   "source": [
    "There is some over fitting on the train set...but in general this model performs very well almost 95% R2 for sentiment classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
