{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import PySpark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets start a spark session\n",
    "spark = SparkSession.builder.appName('classification').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occupancy detection dataset\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../data/OccupancyDetection/data.txt'\n",
    "                    ,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+----------------+-----+----------------+-------------------+---------+\n",
      "|Ndx|               date|Temperature|        Humidity|Light|             CO2|      HumidityRatio|Occupancy|\n",
      "+---+-------------------+-----------+----------------+-----+----------------+-------------------+---------+\n",
      "|  1|2015-02-04 17:51:00|      23.18|          27.272|  426|          721.25|0.00479298817650529|        1|\n",
      "|  2|2015-02-04 17:51:59|      23.15|         27.2675|429.5|             714|0.00478344094931065|        1|\n",
      "|  3|2015-02-04 17:53:00|      23.15|          27.245|  426|           713.5|0.00477946352442199|        1|\n",
      "|  4|2015-02-04 17:54:00|      23.15|            27.2|  426|          708.25|0.00477150882608175|        1|\n",
      "|  5|2015-02-04 17:55:00|       23.1|            27.2|  426|           704.5|0.00475699293331518|        1|\n",
      "|  6|2015-02-04 17:55:59|       23.1|            27.2|  419|             701|0.00475699293331518|        1|\n",
      "|  7|2015-02-04 17:57:00|       23.1|            27.2|  419|701.666666666667|0.00475699293331518|        1|\n",
      "|  8|2015-02-04 17:57:59|       23.1|            27.2|  419|             699|0.00475699293331518|        1|\n",
      "|  9|2015-02-04 17:58:59|       23.1|            27.2|  419|689.333333333333|0.00475699293331518|        1|\n",
      "| 10|2015-02-04 18:00:00|     23.075|          27.175|  419|             688|0.00474535071966655|        1|\n",
      "| 11|2015-02-04 18:01:00|     23.075|           27.15|  419|          690.25|0.00474095189694268|        1|\n",
      "| 12|2015-02-04 18:02:00|       23.1|            27.1|  419|             691|0.00473937073052061|        1|\n",
      "| 13|2015-02-04 18:03:00|       23.1|27.1666666666667|  419|           683.5|0.00475111875560951|        1|\n",
      "| 14|2015-02-04 18:04:00|      23.05|           27.15|  419|           687.5| 0.0047337317970825|        1|\n",
      "| 15|2015-02-04 18:04:59|         23|          27.125|  419|             686|0.00471494214590473|        1|\n",
      "| 16|2015-02-04 18:06:00|         23|          27.125|418.5|           680.5|0.00471494214590473|        1|\n",
      "| 17|2015-02-04 18:07:00|         23|            27.2|    0|           681.5|0.00472807794966877|        0|\n",
      "| 18|2015-02-04 18:08:00|     22.945|           27.29|    0|             685|0.00472795137178073|        0|\n",
      "| 19|2015-02-04 18:08:59|     22.945|           27.39|    0|             685| 0.0047454083970941|        0|\n",
      "| 20|2015-02-04 18:10:00|      22.89|           27.39|    0|             689|0.00472950615591001|        0|\n",
      "+---+-------------------+-----------+----------------+-----+----------------+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ndx: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- Temperature: string (nullable = true)\n",
      " |-- Humidity: string (nullable = true)\n",
      " |-- Light: string (nullable = true)\n",
      " |-- CO2: string (nullable = true)\n",
      " |-- HumidityRatio: string (nullable = true)\n",
      " |-- Occupancy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printSchema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the data to the types we want\n",
    "from pyspark.sql.types import (StructField, \n",
    "                               StringType, \n",
    "                               IntegerType,\n",
    "                               DateType,\n",
    "                               DoubleType,\n",
    "                               StructType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField('Ndx', StringType(), True), \n",
    "               StructField('date', StringType(), True),\n",
    "               StructField('Temperature', DoubleType(), True),\n",
    "               StructField('Humidity', DoubleType(), True),\n",
    "               StructField('Light', DoubleType(), True),\n",
    "               StructField('CO2', DoubleType(), True),\n",
    "               StructField('HumidityRatio', DoubleType(), True),\n",
    "               StructField('Occupancy', IntegerType(), True)\n",
    "              ]\n",
    "final_struct = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the data again with correct data types in schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../data/OccupancyDetection/data.txt'\n",
    "                    ,header=True\n",
    "                    ,schema=final_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+----------------+-----+----------------+-------------------+---------+\n",
      "|Ndx|               date|Temperature|        Humidity|Light|             CO2|      HumidityRatio|Occupancy|\n",
      "+---+-------------------+-----------+----------------+-----+----------------+-------------------+---------+\n",
      "|  1|2015-02-04 17:51:00|      23.18|          27.272|426.0|          721.25|0.00479298817650529|        1|\n",
      "|  2|2015-02-04 17:51:59|      23.15|         27.2675|429.5|           714.0|0.00478344094931065|        1|\n",
      "|  3|2015-02-04 17:53:00|      23.15|          27.245|426.0|           713.5|0.00477946352442199|        1|\n",
      "|  4|2015-02-04 17:54:00|      23.15|            27.2|426.0|          708.25|0.00477150882608175|        1|\n",
      "|  5|2015-02-04 17:55:00|       23.1|            27.2|426.0|           704.5|0.00475699293331518|        1|\n",
      "|  6|2015-02-04 17:55:59|       23.1|            27.2|419.0|           701.0|0.00475699293331518|        1|\n",
      "|  7|2015-02-04 17:57:00|       23.1|            27.2|419.0|701.666666666667|0.00475699293331518|        1|\n",
      "|  8|2015-02-04 17:57:59|       23.1|            27.2|419.0|           699.0|0.00475699293331518|        1|\n",
      "|  9|2015-02-04 17:58:59|       23.1|            27.2|419.0|689.333333333333|0.00475699293331518|        1|\n",
      "| 10|2015-02-04 18:00:00|     23.075|          27.175|419.0|           688.0|0.00474535071966655|        1|\n",
      "| 11|2015-02-04 18:01:00|     23.075|           27.15|419.0|          690.25|0.00474095189694268|        1|\n",
      "| 12|2015-02-04 18:02:00|       23.1|            27.1|419.0|           691.0|0.00473937073052061|        1|\n",
      "| 13|2015-02-04 18:03:00|       23.1|27.1666666666667|419.0|           683.5|0.00475111875560951|        1|\n",
      "| 14|2015-02-04 18:04:00|      23.05|           27.15|419.0|           687.5| 0.0047337317970825|        1|\n",
      "| 15|2015-02-04 18:04:59|       23.0|          27.125|419.0|           686.0|0.00471494214590473|        1|\n",
      "| 16|2015-02-04 18:06:00|       23.0|          27.125|418.5|           680.5|0.00471494214590473|        1|\n",
      "| 17|2015-02-04 18:07:00|       23.0|            27.2|  0.0|           681.5|0.00472807794966877|        0|\n",
      "| 18|2015-02-04 18:08:00|     22.945|           27.29|  0.0|           685.0|0.00472795137178073|        0|\n",
      "| 19|2015-02-04 18:08:59|     22.945|           27.39|  0.0|           685.0| 0.0047454083970941|        0|\n",
      "| 20|2015-02-04 18:10:00|      22.89|           27.39|  0.0|           689.0|0.00472950615591001|        0|\n",
      "+---+-------------------+-----------+----------------+-----+----------------+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ndx: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Humidity: double (nullable = true)\n",
      " |-- Light: double (nullable = true)\n",
      " |-- CO2: double (nullable = true)\n",
      " |-- HumidityRatio: double (nullable = true)\n",
      " |-- Occupancy: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ndx: string (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Humidity: double (nullable = true)\n",
      " |-- Light: double (nullable = true)\n",
      " |-- CO2: double (nullable = true)\n",
      " |-- HumidityRatio: double (nullable = true)\n",
      " |-- Occupancy: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert Date to dateType in PySpark\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "df = df.withColumn('New_datetime', to_timestamp(df['date'],format='yyyy-MM-dd HH:mm:SS'))\n",
    "\n",
    "#Drop old date column\n",
    "df = df.drop(df['date'])\n",
    "#rename New_date as date\n",
    "df = df.withColumnRenamed('New_datetime', 'date')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import vector and VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assemble the features vector\n",
    "assembler = VectorAssembler(inputCols=['Temperature',\n",
    "                                       'Humidity',\n",
    "                                       'Light',\n",
    "                                       'CO2', \n",
    "                                       'HumidityRatio'], \n",
    "                           outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[23.18,27.272,426...|    1|\n",
      "|[23.15,27.2675,42...|    1|\n",
      "|[23.15,27.245,426...|    1|\n",
      "|[23.15,27.2,426.0...|    1|\n",
      "|[23.1,27.2,426.0,...|    1|\n",
      "|[23.1,27.2,419.0,...|    1|\n",
      "|[23.1,27.2,419.0,...|    1|\n",
      "|[23.1,27.2,419.0,...|    1|\n",
      "|[23.1,27.2,419.0,...|    1|\n",
      "|[23.075,27.175,41...|    1|\n",
      "|[23.075,27.15,419...|    1|\n",
      "|[23.1,27.1,419.0,...|    1|\n",
      "|[23.1,27.16666666...|    1|\n",
      "|[23.05,27.15,419....|    1|\n",
      "|[23.0,27.125,419....|    1|\n",
      "|[23.0,27.125,418....|    1|\n",
      "|[23.0,27.2,0.0,68...|    0|\n",
      "|[22.945,27.29,0.0...|    0|\n",
      "|[22.945,27.39,0.0...|    0|\n",
      "|[22.89,27.39,0.0,...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final = output.select(['features', 'Occupancy'])\n",
    "#Rename the Occupancy as label\n",
    "df_final = df_final.withColumnRenamed('Occupancy', 'label')\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "train_data, test_data = df_final.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr_model = lr.fit(train_data)\n",
    "test_result = lr_model.transform(test_data)\n",
    "train_result = lr_model.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import multi class classification evaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model\n",
    "me = MulticlassClassificationEvaluator(metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result = 0.983\n",
      "Train result = 0.988\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test result = {me.evaluate(test_result):0.3}\")\n",
    "print(f\"Train result = {me.evaluate(train_result):0.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "lr = LinearSVC()\n",
    "lr_model = lr.fit(train_data)\n",
    "test_result = lr_model.transform(test_data)\n",
    "train_result = lr_model.transform(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result = 0.985\n",
      "Train result = 0.989\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "me = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "print(f\"Test result = {me.evaluate(test_result):0.3}\")\n",
    "print(f\"Train result = {me.evaluate(train_result):0.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result = 0.987\n",
      "Train result = 0.992\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc_model = dtc.fit(train_data)\n",
    "test_result = dtc_model.transform(test_data)\n",
    "train_result = dtc_model.transform(train_data)\n",
    "\n",
    "#Evaluate the model\n",
    "me = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "print(f\"Test result = {me.evaluate(test_result):0.3}\")\n",
    "print(f\"Train result = {me.evaluate(train_result):0.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result = 0.987\n",
      "Train result = 0.991\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "dtc = RandomForestClassifier(numTrees=100)\n",
    "dtc_model = dtc.fit(train_data)\n",
    "test_result = dtc_model.transform(test_data)\n",
    "train_result = dtc_model.transform(train_data)\n",
    "\n",
    "#Evaluate the model\n",
    "me = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "print(f\"Test result = {me.evaluate(test_result):0.3}\")\n",
    "print(f\"Train result = {me.evaluate(train_result):0.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result = 0.991\n",
      "Train result = 0.996\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "dtc = GBTClassifier()\n",
    "dtc_model = dtc.fit(train_data)\n",
    "test_result = dtc_model.transform(test_data)\n",
    "train_result = dtc_model.transform(train_data)\n",
    "\n",
    "#Evaluate the model\n",
    "me = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "print(f\"Test result = {me.evaluate(test_result):0.3}\")\n",
    "print(f\"Train result = {me.evaluate(train_result):0.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see Gradient boosted trees performed the best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
